
# üìù Publications 
## ‚úçÔ∏è Controllable Image Generation

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025 Oral</div><img src='images/anyedit.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AnyEdit Unified High-Quality Image Edit with Any Idea](https://arxiv.org/pdf/2411.15738) \\
**Qifan Yu***, Wei Chow*, Zhongqi Yue*, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, Yueting Zhuang

[**Project**](https://dcd-anyedit.github.io/)

- We present a comprehensive multi-modal image editing dataset, AnyEdit, to address the scarcity of high-quality instruction editing data for controllable image generation. This repository contains the official implementation, models, datasets, and data toolkit for the pipeline.
- AnyEdit comprises 2.5 million high-quality editing pairs spanning 25 editing types for the community, and achieves SOTA results on numerous editing benchmarks.
- Find out the official [datasets](https://huggingface.co/datasets/Bin1117/AnyEdit) and pre-trained checkpoint [AnySD](https://huggingface.co/WeiChow/AnySD).
- We also developed and open-sourced an [Benchmark](https://drive.google.com/file/d/1V-Z4agWoTMzAYkRJQ1BNz0-i79eAVWt4/view?usp=sharing) to evaluate all baselines and our model more comprehensively.
</div>
</div>

- ``Preprint`` [Interactive data synthesis for systematic vision adaptation via llms-aigcs collaboration](https://arxiv.org/pdf/2305.12799), **Qifan Yu**, Juncheng Li, Wentao Ye, Siliang Tang, Yueting Zhuang, [Code](https://github.com/Yuqifan1117/Labal-Anything-Pipeline)

- ``Under Review`` [Dancing avatar: Pose and text-guided human motion videos synthesis with image diffusion model](https://arxiv.org/pdf/2308.07749), Bosheng Qin, Wentao Ye, **Qifan Yu**, Siliang Tang, Yueting Zhuang

- ``Under Review`` [SOYO: A Tuning-Free Approach for Video Style Morphing via Style-Adaptive Interpolation in Diffusion Models](https://arxiv.org/pdf/2503.06998), Haoyu Zheng, **Qifan Yu**, Binghe Yu, Yang Dai, Wenqiao Zhang, Juncheng Li, Siliang Tang, Yueting Zhuang

## üôÜ Vision-languag Understanding
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/hallucidoctor.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data](https://arxiv.org/pdf/2311.13614) \\
**Qifan Yu**, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, Yueting Zhuang

[**Project**](https://github.com/Yuqifan1117/HalluciDoctor)

HalluciDoctor is the first hallucination mitigating framework for the hallucinatory toxicity in MLLM datasets (LLaVA et al.).
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/datatailor.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Mastering Collaborative Multi-modal Data Selection: A Focus on Informativeness, Uniqueness, and Representativeness](https://arxiv.org/pdf/2412.06293) \\
**Qifan Yu***, Zhebei Shen*, Zhongqi Yue*, Yang Wu, Wenqiao Zhang, Yunfei Li, Juncheng Li, Siliang Tang, Yueting Zhuang

[**Project**](https://github.com/Yuqifan1117/DataTailor)

DataTailor provides a principled and interpretable way for multi-modal data selection, enabling 85% training cost saving for SFT!
</div>
</div>

- ``CVPR 2025`` [STEP: Enhancing Video-LLMs' Compositional Reasoning by Spatio-Temporal Graph-guided Self-Training](https://openaccess.thecvf.com/content/CVPR2025/papers/Qiu_STEP_Enhancing_Video-LLMs_Compositional_Reasoning_by_Spatio-Temporal_Graph-guided_Self-Training_CVPR_2025_paper.pdf), Haiyi Qiu, Minghe Gao, Long Qian, Kaihang Pan, **Qifan Yu**, Juncheng Li, Wenjie Wang, Siliang Tang, Yueting Zhuang, Tat-Seng Chua

- ``ICCV 2023`` [Visually-prompted language model for fine-grained scene graph generation in an open world](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Visually-Prompted_Language_Model_for_Fine-Grained_Scene_Graph_Generation_in_an_ICCV_2023_paper.pdf), **Qifan Yu**, Juncheng Li, Yu Wu, Siliang Tang, Wei Ji, Yueting Zhuang

- ``NeurIPS 2024`` [Unified Generative and Discriminative Training for Multi-modal Large Language Models](https://arxiv.org/pdf/2411.00304), Wei Chow, Juncheng Li, **Qifan Yu**, Kaihang Pan, Hao Fei, Zhiqi Ge, Shuai Yang, Siliang Tang, Hanwang Zhang, Qianru Sun

- ``NeurIPS 2024 Spotlight`` [Towards unified multimodal editing with enhanced knowledge collaboration
](https://proceedings.neurips.cc/paper_files/paper/2024/file/c705ba25f183b875c9359ef83fa262e8-Paper-Conference.pdf), Kaihang Pan, Zhaoyu Fan, Juncheng Li, **Qifan Yu**, Hao Fei, Siliang Tang, Richang Hong, Hanwang Zhang, Qianru Sun
## ü§ñ Visual Agent
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2025 Oral</div><img src='images/hallucidoctor.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities](https://arxiv.org/pdf/2506.08933?) \\
Wendong Bu*, Yang Wu*, **Qifan Yu***, Minghe Gao, Bingchen Miao, Zhenkui Zhang, Kaihang Pan, Yunfei Li, Mengze Li, Wei Ji, Juncheng Li, Siliang Tang, Yueting Zhuang

[**Project**](https://omni-bench.github.io/)

- We propose a novel self-generating, graph-based benchmark, OmniBench, for comprehensive agent evaluation at multiple granularities.
- OmniBench contains 36k graph-structured tasks across 20 scenarios, achieving a 91% human acceptance rate. We show a promising [**Demo**](https://youtu.be/ajhFH4rIN3w) to show its performance across various capabilities and paving the way for future advancements.
</div>
</div>

- ``ICML 2025`` [Boosting Virtual Agent Learning and Reasoning: A Step-wise, Multi-dimensional, and Generalist Reward Model with Benchmark](https://arxiv.org/pdf/2503.18665), Bingchen Miao, Yang Wu, Minghe Gao, **Qifan Yu**, Wendong Bu, Wenqiao Zhang, Yunfei Li, Siliang Tang, Tat-Seng Chua, Juncheng Li
